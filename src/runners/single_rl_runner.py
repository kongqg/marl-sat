import collections
import csv
import os
import random
import sys

import chex
from flax.core import freeze, unfreeze
from hydra.utils import to_absolute_path

os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'

# =========================================================
import jax

print("--- JAX ç¯å¢ƒè¯Šæ–­ ---")
try:
    print(f"JAX é»˜è®¤åç«¯: {jax.default_backend()}")
    print(f"JAX å¯ç”¨è®¾å¤‡: {jax.devices()}")
except Exception as e:
    print(f"æ— æ³•è·å– JAX è®¾å¤‡ä¿¡æ¯: {e}")
print("----------------------\n")
# =========================================================

import jax.numpy as jnp
import numpy as np
import distrax
from tqdm import tqdm
from flax.training import checkpoints
from datetime import datetime
import hydra
from omegaconf import DictConfig

# ä½ è‡ªå·±çš„æ¨¡å—
from src.envs.sat_env import SatEnv
from src.models.ac_gnn import ACGNN
from src.learners import single_rl_learner
from src.utils.data_parser import parse_cnf
from src.utils.metrics_tools import flatten_metrics, mean_std
import logging
logging.getLogger("absl").setLevel(logging.WARNING)
# å¯¼å…¥ä¼˜åŒ–æ‰€éœ€çš„é¢å¤–æ¨¡å—
import flax.struct
from typing import NamedTuple, Any
from functools import partial

# ========================
# è¶…å‚ï¼šå¾ªç¯å¼è®­ç»ƒ & è¯„ä¼°
# ========================
TRAIN_STEPS_PER_CYCLE = 3000   # æ¯ä¸ªå¾ªç¯ç”¨äºè®­ç»ƒçš„æ•°æ®æ­¥æ•°ï¼ˆrollout stepsï¼‰
NUM_ENVS = 4  # <-- æ–°å¢ï¼šå¹¶è¡Œç¯å¢ƒçš„æ•°é‡
EVAL_EPISODES_PER_CYCLE = 50  # æ¯ä¸ªå¾ªç¯è¯„ä¼°çš„episodesæ•°é‡
# NUM_CYCLES = 500  # å¾ªç¯æ¬¡æ•°
SAVE_BEST_EVERY_CYCLE = True  # æ¯ä¸ªå¾ªç¯ç»“æŸæ ¹æ®evalæœ€ä¼˜å¦å­˜best.ckpt
LOG_FILE = "train_eval_log.txt"


def load_cnf_problems(cnf_data_dir: str):
    cnf_fnames = sorted([f for f in os.listdir(cnf_data_dir) if f.endswith('.cnf')])
    problems = []
    print(f"Found {len(cnf_fnames)} SAT instances.")
    for fname in tqdm(cnf_fnames, desc="LOADING CNF PROBLEMS"):
        cnf_path = os.path.join(cnf_data_dir, fname)
        num_vars, num_clauses, clauses = parse_cnf(cnf_path)
        problems.append({
            "num_vars": num_vars,
            "num_clauses": num_clauses,
            "clauses": clauses,
        })
    return problems


def set_global_seeds(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    # jax æœ¬èº«ç”¨ PRNGKeyï¼Œä¸æ˜¯å…¨å±€éšæœºçŠ¶æ€
    key = jax.random.PRNGKey(seed)
    return key


# ==============================================================================
#                      ã€æ”¹é€ å¾Œçš„æ ¸å¿ƒä¸¦è¡Œå‡½æ•¸ã€‘
# ==============================================================================
@flax.struct.dataclass
class VmappedRolloutCarryState:
    """ç”¨æ–¼åœ¨ lax.scan ä¸­å‚³éä¸¦è¡Œç’°å¢ƒç‹€æ…‹çš„å®¹å™¨ã€‚"""
    train_state: flax.training.train_state.TrainState
    vmapped_env_state: Any
    vmapped_obs: Any
    key: jax.random.PRNGKey
    # è¿½è¸ªæ¯ä¸ªå¹¶è¡Œç¯å¢ƒçš„å›åˆæ•°
    ep_counts: chex.Array

@partial(jax.jit, static_argnames=("network", "vmapped_reset_fn", "vmapped_step_fn", "num_steps", "num_envs"))
def collect_rollouts(
        initial_carry_state: VmappedRolloutCarryState,
        problems,
        network: ACGNN,
        vmapped_reset_fn,
        vmapped_step_fn,
        num_steps: int,
        num_envs: int
):
    """
    ã€JAX ä¸¦è¡Œå„ªåŒ–ç‰ˆã€‘
    ä½¿ç”¨ jax.lax.scan åœ¨æ™‚é–“ç¶­åº¦ä¸Šè¿­ä»£ï¼Œè€Œåœ¨æ¯å€‹æ™‚é–“æ­¥å…§éƒ¨ï¼Œ
    æ‰€æœ‰ç’°å¢ƒ (num_envs) éƒ½è¢«ä¸¦è¡Œè™•ç†ã€‚
    """

    def _one_step_rollout(carry, _):
        """å®šç¾©å–®æ­¥æ“ä½œï¼Œä½†é€™æ¬¡æ˜¯é‡å°ä¸€æ‰¹ (vmapped) ç’°å¢ƒ"""
        ep_counts = carry.ep_counts  # <-- è§£åŒ…æ–°å¢çš„è®¡æ•°å™¨
        train_state = carry.train_state
        vmapped_env_state = carry.vmapped_env_state
        vmapped_obs = carry.vmapped_obs
        key = carry.key

        # a. å‹•ä½œæ¡æ¨£ (æ¨¡å‹åŸç”Ÿæ”¯æŒ batch è¼¸å…¥)
        key, act_key = jax.random.split(key)
        logits, value = network.apply({'params': train_state.params}, vmapped_obs['agent_0'])
        pi = distrax.Categorical(logits=logits)
        actions = pi.sample(seed=act_key)
        log_probs = pi.log_prob(actions)
        # b. ç’°å¢ƒä¸¦è¡Œæ­¥é€²
        key, step_key = jax.random.split(key)
        step_keys = jax.random.split(step_key, num_envs)
        actions_dict = {"agent_0": actions}
        next_vmapped_obs, next_vmapped_env_state, rewards, dones, infos = vmapped_step_fn(
            step_keys, vmapped_env_state, actions_dict
        )

        # c. è™•ç†å®Œæˆçš„å›åˆ (è‡ªå‹•é‡ç½®)
        key, reset_key = jax.random.split(key)
        reset_keys = jax.random.split(reset_key, num_envs)
        N = problems["num_vars"].shape[0]
        key, idx_key = jax.random.split(key)
        new_problem_indices = jax.random.randint(idx_key, (num_envs,), 0, N)
        new_problems = jax.tree_util.tree_map(lambda x: x[new_problem_indices], problems)
        obs_after_reset, state_after_reset = vmapped_reset_fn(reset_keys, new_problems)

        done_mask = dones["__all__"]
        new_ep_counts = ep_counts + done_mask.astype(jnp.int32)

        def _reset_if_done(old, new):
            """æ ¹æ“š done_mask é¸æ“‡æ€§åœ°æ›¿æ› PyTree çš„è‘‰ç¯€é»"""
            # old.ndim æ˜¯èˆŠç‹€æ…‹é™£åˆ—çš„ç¶­åº¦æ•¸, e.g., A_pos æ˜¯ 3
            # æˆ‘å€‘éœ€è¦ç‚º done_mask å¢åŠ  (ndim - 1) å€‹æ–°ç¶­åº¦
            # ä¾‹å¦‚ï¼Œå°æ–¼ A_pos (ndim=3)ï¼Œmask shape è®Šç‚º (16, 1, 1)
            mask_shape = done_mask.shape + (1,) * (old.ndim - 1)
            reshaped_mask = jnp.reshape(done_mask, mask_shape)
            return jnp.where(reshaped_mask, new, old)

        final_next_env_state = jax.tree_util.tree_map(
            _reset_if_done, next_vmapped_env_state, state_after_reset
        )
        final_next_obs = jax.tree_util.tree_map(
            _reset_if_done, next_vmapped_obs, obs_after_reset
        )

        # d. å­˜å„² Transition
        transition = single_rl_learner.Transition(
            done=dones["__all__"],
            action=actions,
            value=value,
            reward=rewards["agent_0"],
            log_prob=log_probs,
            obs=vmapped_obs['agent_0'],  # å­˜å‚¨çš„æ˜¯æ­¥è¿›å‰çš„ obs (s_t)
            info=infos
        )

        next_carry = VmappedRolloutCarryState(
            train_state=train_state,
            vmapped_env_state=final_next_env_state,  # ä½¿ç”¨é‡ç½®åçš„çŠ¶æ€
            vmapped_obs=final_next_obs,  # ä½¿ç”¨é‡ç½®åçš„è§‚æµ‹
            key=key,
            ep_counts=new_ep_counts
        )

        return next_carry, transition

    # ä½¿ç”¨ jax.lax.scan åŸ·è¡Œæ•´å€‹ rollout å¾ªç’°
    final_carry, traj_batch = jax.lax.scan(
        _one_step_rollout, initial_carry_state, None, length=num_steps
    )

    # è¨ˆç®—æœ€å¾Œä¸€å€‹ç‹€æ…‹çš„åƒ¹å€¼
    _, last_val = network.apply({'params': final_carry.train_state.params}, final_carry.vmapped_obs['agent_0'])

    return final_carry, traj_batch, last_val


@partial(jax.jit, static_argnames=("network", "env", "max_steps"))
def evaluate_policy(
        key, env, network, params, problems,
        problem_indices,  #
        max_steps: int
):
    """
    ã€æ–°ç‰ˆã€‘è¯„ä¼°ç­–ç•¥ï¼šå¯¹æŒ‡å®šçš„ä¸€æ‰¹é—®é¢˜è¿›è¡Œè¯„ä¼°ã€‚
    """

    def _run_one_episode(key, problem_idx):
        """å†…éƒ¨å‡½æ•°ï¼Œè¿è¡Œå•ä¸ªå›åˆï¼Œé€»è¾‘ä¿æŒä¸å˜"""
        problem = jax.tree_util.tree_map(lambda x: x[problem_idx], problems)
        key, reset_key = jax.random.split(key)
        obs, state = env.reset(reset_key, problem)

        def _one_step_eval(carry, _):
            obs, state, key = carry
            logits, _ = network.apply({'params': params}, obs['agent_0'])
            action = jnp.argmax(logits, axis=-1)
            key, step_key = jax.random.split(key)
            next_obs, next_state, reward, done, info = env.step_env(
                step_key, state, {"agent_0": action}
            )
            return (next_obs, next_state, key), (reward, done)

        _, (rewards, dones) = jax.lax.scan(
            _one_step_eval, (obs, state, key), None, length=max_steps
        )

        first_done_idx = jnp.argmax(dones["__all__"])
        ep_len = jnp.where(jnp.any(dones["__all__"]), first_done_idx + 1, max_steps)
        step_indices = jnp.arange(max_steps)
        mask = step_indices < ep_len
        ep_return = jnp.sum(rewards["agent_0"] * mask)
        terminal_reward = jnp.sum(rewards["agent_0"] * dones["__all__"])
        is_solved = terminal_reward > 0
        return is_solved, ep_return, ep_len

    # --- æ ¸å¿ƒä¿®æ”¹ ---
    # ä½¿ç”¨ä¼ å…¥çš„ problem_indicesï¼Œè€Œä¸æ˜¯éšæœºç”Ÿæˆ
    keys = jax.random.split(key, problem_indices.shape[0])

    # vmap ç°åœ¨ä¼šéå†æˆ‘ä»¬æä¾›çš„æ‰€æœ‰é—®é¢˜ç´¢å¼•
    solves, ep_returns, ep_lens = jax.vmap(_run_one_episode)(keys, problem_indices)

    eval_stats = {
        "eval_solve_rate": jnp.mean(solves),
        "eval_avg_len": jnp.mean(ep_lens),
        "eval_avg_return": jnp.mean(ep_returns),
        "eval_episodes": len(solves),
    }
    # è¿”å›æ›´æ–°åçš„ key å’Œç»Ÿè®¡æ•°æ®
    return jax.random.split(key)[0], eval_stats


def reset_heads_to_init(train_state, network, env, problems_raw, seed=42, reset_actor=True, reset_critic=True):
    """
    (æ­¤å‡½æ•¸ä¿æŒä¸è®Š)
    """
    init_key = jax.random.PRNGKey(seed)
    dummy_obs, _ = env.reset(init_key, problems_raw[0])
    blank_params = network.init(init_key, dummy_obs['agent_0'])['params']
    p = unfreeze(train_state.params)
    np_ = unfreeze(blank_params)
    if reset_actor:
        if 'actor_dense_1' in p:
            p['actor_dense_1'] = np_['actor_dense_1']
            if 'actor_dense_2' in p:
                p['actor_dense_2'] = np_['actor_dense_2']
        if 'actor_output' in p:
            p['actor_output'] = np_['actor_output']
    if reset_critic:
        if 'critic_dense_1' in p:
            p['critic_dense_1'] = np_['critic_dense_1']
        if 'critic_dense_2' in p:
            p['critic_dense_2'] = np_['critic_dense_2']
        if 'critic_output' in p:
            p['critic_output'] = np_['critic_output']
    new_params = freeze(p)
    new_opt_state = train_state.tx.init(new_params)
    train_state = train_state.replace(params=new_params, opt_state=new_opt_state, step=0)
    print("âœ… Reinitialized actor/critic heads successfully.")
    return train_state


@hydra.main(version_base=None, config_path="../../configs", config_name="single_rl_ppo_config.yaml")
def train(hydra_config: DictConfig):
    global config
    config = hydra_config

    # 0) RNG & æ•°æ®
    key = set_global_seeds(config.SEED)
    problems_raw = load_cnf_problems(config.ENV_PARAMS.CNF_DATA_DIR) # shape( num_ problems_raw )
    problems = jax.tree_util.tree_map(lambda *x: jnp.stack(x), *problems_raw)
    # 1) ç¯å¢ƒ/æ¨¡å‹
    num_vars = problems_raw[0]["num_vars"]
    num_clauses = problems_raw[0]["num_clauses"]
    env = SatEnv(num_vars, num_clauses, max_clause_len=config.ENV_PARAMS.WRAPPER_PARAMS.max_clause_len,
                 c_bonus=config.ENV_PARAMS.WRAPPER_PARAMS.c_bonus,
                 alpha=config.ENV_PARAMS.WRAPPER_PARAMS.alpha
                 ,max_steps=config.ENV_PARAMS.WRAPPER_PARAMS.max_steps)

    # --- æ”¹é€ é» 1ï¼šå‰µå»ºå‘é‡åŒ–çš„ç’°å¢ƒå‡½æ•¸ ---
    vmapped_reset = jax.vmap(env.reset, in_axes=(0, 0))
    vmapped_step = jax.vmap(env.step_env, in_axes=(0, 0, 0))

    network = ACGNN(
        hidden_dim=config.MODEL_PARAMS.HIDDEN_DIM,
        num_message_passing_step=config.MODEL_PARAMS.NUM_MESSAGE_PASSING_STEP
    )

    # 2) åˆå§‹åŒ– TrainState
    key, net_key = jax.random.split(key)
    dummy_obs, _ = env.reset(jax.random.PRNGKey(0), problems_raw[0])
    train_state = single_rl_learner.create_train_state(
        model=network,
        key=net_key,
        config=config,
        dummy_input=dummy_obs['agent_0']
    )

    # åŠ è½½modelï¼ˆåªæ¢å¤ paramsï¼‰
    if config.TRAIN_PARAMS.RESUME_CKPT_PATH is not None:
        resume_dir_abs = to_absolute_path(config.TRAIN_PARAMS.RESUME_CKPT_PATH)

        try:
            print(f"ğŸ”„ Attempting to resume from checkpoint: {resume_dir_abs}")

            # (1) æ¢å¾©å®Œæ•´çš„ TrainState (åŒ…å« params, opt_state, step)
            restored_state = checkpoints.restore_checkpoint(
                ckpt_dir=resume_dir_abs,
                target=train_state,  # é€™è£¡ target åªæ˜¯ç‚ºäº†çµæ§‹åŒ¹é…
                prefix="cycle_3000"  # æˆ–è€… "cycle_"
            )

            # (2) åªä¿ç•™æ¢å¾©çš„ paramsï¼Œæ‹‹æ£„èˆŠçš„ opt_state å’Œ step
            #    é€™å°±éš±å¼åœ°é‡ç½®äº†å„ªåŒ–å™¨ï¼Œå› ç‚ºæˆ‘å€‘å°‡ä½¿ç”¨ä¸‹é¢æ–°åˆå§‹åŒ–çš„ opt_state
            train_state = train_state.replace(params=restored_state.params)
            print("âœ… Successfully restored model parameters (GNN body).")

            # (3) é‡ç½® Actor/Critic çš„é ­éƒ¨ï¼Œä¸¦é‡æ–°åˆå§‹åŒ–å„ªåŒ–å™¨ç‹€æ…‹
            train_state = reset_heads_to_init(
                train_state=train_state,
                network=network,
                env=env,
                problems_raw=problems_raw,
                seed=config.SEED,  # ä½¿ç”¨å›ºå®šçš„ç¨®å­ä¿è­‰é‡ç½®çš„ä¸€è‡´æ€§
                reset_actor=True,
                reset_critic=True
            )
            print("ğŸ”§ Reinitialized Actor/Critic heads and Optimizer state for the new curriculum stage.")

        except Exception as e:
            print(f"âš ï¸ Resume failed ({e}), starting training from scratch.")

    # 3) æ—¥å¿— & Checkpoint (ä¿æŒä¸è®Š)
    time_str = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    ckpt_dir = os.path.abspath(f"experiments/single_rl/{time_str}/checkpoints")
    os.makedirs(ckpt_dir, exist_ok=True)
    print(f"Checkpoints will be saved in: {ckpt_dir}")

    log_path = os.path.join(os.path.dirname(ckpt_dir), LOG_FILE)
    with open(log_path, "w", encoding="utf-8") as f:
        f.write(f"# Train/Eval Log | start_time={time_str} | seed={config.SEED}\n")
        f.write("# Fields: cycle | "
                "train_loss_total train_loss_value train_loss_policy train_entropy | "
                "train_value_mean train_value_std train_reward_mean train_reward_std | "
                "train_ep_num train_solve_rate train_avg_len train_avg_return | "
                "eval_solve_rate eval_avg_len eval_avg_return eval_episodes\n")

    ppo_update = single_rl_learner.make_update(config.PPO_PARAMS)
    best_solve = -1.0
    best_len = float('inf')

    # 4) å¾ªç’°å¼è¨“ç·´-è¯„ä¼°

    pbar = tqdm(range(1, config.TRAIN_PARAMS.NUM_CYCLES + 1), desc="Cycles")
    steps_per_env = TRAIN_STEPS_PER_CYCLE // NUM_ENVS

    for cycle_idx in pbar:
        key, p_idx_key, reset_key, rollout_key, update_key = jax.random.split(key, 5)

        # 1. ä¸ºæœ¬æ¬¡ rollout éšæœºé€‰æ‹©ä¸€æ‰¹åˆå§‹é—®é¢˜
        initial_problem_indices = jax.random.randint(p_idx_key, (NUM_ENVS,), 0, len(problems_raw))
        initial_problems = jax.tree_util.tree_map(
            lambda *x: jnp.stack(x), *[problems_raw[i] for i in initial_problem_indices]
        )



        # 2. ç”¨æ–°çš„ key é‡ç½®æ‰€æœ‰ç¯å¢ƒï¼Œç¡®ä¿åˆå§‹çŠ¶æ€çš„éšæœºæ€§
        reset_keys = jax.random.split(reset_key, NUM_ENVS)
        vmapped_obs, vmapped_state = vmapped_reset(reset_keys, initial_problems)

        # 3. ä¸ºæœ¬æ¬¡ rollout æ„å»ºä¸€ä¸ªå…¨æ–°çš„ã€å¹²å‡€çš„åˆå§‹ carry å®¹å™¨
        #    æ³¨æ„ï¼štrain_state æ˜¯ä»ä¸Šä¸€ä¸ªå¾ªç¯ç»§æ‰¿çš„ï¼Œå› ä¸ºå®ƒåŒ…å«äº†éœ€è¦æŒç»­å­¦ä¹ çš„æ¨¡å‹æƒé‡
        carry = VmappedRolloutCarryState(
            train_state=train_state,
            vmapped_env_state=vmapped_state,
            vmapped_obs=vmapped_obs,
            key=rollout_key,
            ep_counts=jnp.ones(NUM_ENVS, dtype=jnp.int32)
        )

        # ===== 1. å¹¶è¡Œæ”¶é›†æ•°æ® (Collect Rollouts) =====
        # ç°åœ¨ collect_rollouts å‡½æ•°æ¥æ”¶çš„æ˜¯ä¸€ä¸ªå¹²å‡€çš„åˆå§‹çŠ¶æ€
        ep_counts_before = jnp.sum(carry.ep_counts)
        carry, traj_batch, last_val = collect_rollouts(
            carry, initial_problems, network, vmapped_reset, vmapped_step,
            num_steps=steps_per_env, num_envs=NUM_ENVS,
        )
        ep_counts_after = jnp.sum(carry.ep_counts)
        # æœ¬æ¬¡ rollout ä¸­æ€»å…±å¼€å§‹è¿‡çš„å›åˆæ•° = ç»“æŸæ—¶çš„æ€»æ•° - å¼€å§‹æ—¶çš„æ€»æ•° + åˆå§‹ç¯å¢ƒæ•°
        # (å› ä¸ºåˆå§‹çš„ NUM_ENVS ä¸ªå›åˆä¹Ÿç®—åœ¨å†…)
        total_episodes_started = (ep_counts_after - ep_counts_before) + NUM_ENVS

        # è¿™é‡Œçš„3åªæ˜¯åˆ¤æ–­ æ˜¯å¦æ­£çš„å®Œæˆäº†
        solved_in_train = jnp.sum((traj_batch.reward > 3) & traj_batch.done)
        train_solve_rate = jax.lax.cond(
            total_episodes_started > 0,
            lambda: solved_in_train / total_episodes_started,
            lambda: 0.0
        )
        # ä»è¿”å›çš„ carry ä¸­è·å–æ›´æ–°åçš„ keyï¼Œä»¥ä¾¿ä¸‹ä¸€æ¬¡å¾ªç¯ä½¿ç”¨
        key = carry.key

        # ===== 2. æ›´æ–°æ¨¡å‹ (PPO Update) =====
        key, update_key = jax.random.split(key)
        # è¿™é‡Œçš„ carry.train_state æ˜¯ rollout ç»“æŸæ—¶çš„ train_stateï¼Œä½†å®é™…ä¸Šå®ƒåœ¨ rollout æœŸé—´æ²¡æœ‰æ”¹å˜
        # æˆ‘ä»¬ä½¿ç”¨è¿™ä¸ª train_state æ¥è¿›è¡Œæ›´æ–°
        updated_train_state, metrics = ppo_update(
            network, carry.train_state, traj_batch, last_val, update_key
        )

        # å°†æ›´æ–°åçš„ train_state ä¿å­˜ä¸‹æ¥ï¼Œç”¨äºä¸‹ä¸€ä¸ªå¾ªç¯
        train_state = updated_train_state

        # ===== 3. è®¡ç®—å¹¶è®°å½•è®­ç»ƒç»Ÿè®¡æ•°æ® (Logging) =====
        metrics_py = jax.device_get(metrics)
        total_loss = np.mean(metrics_py[0])
        vloss = np.mean(metrics_py[1])  # <-- ä¿®æ­£ï¼šç›´æ¥å– metrics_py[1]
        aloss = np.mean(metrics_py[2])  # <-- ä¿®æ­£ï¼šç›´æ¥å– metrics_py[2]
        ent = np.mean(metrics_py[3])

        val_mean, val_std = mean_std(traj_batch.value)
        rew_mean, rew_std = mean_std(traj_batch.reward)

        # final_env_state = carry.vmapped_env_state
        # num_solved_in_batch = jnp.sum(final_env_state.prev_unsat_ratio == 0.0)
        # train_solve_rate = num_solved_in_batch / NUM_ENVS
        tr_stats_py = jax.device_get({
            "loss_total": total_loss, "loss_value": vloss, "loss_policy": aloss, "entropy": ent,
            "value_mean": val_mean, "value_std": val_std, "reward_mean": rew_mean, "reward_std": rew_std,
            "train_ep_num": total_episodes_started,  # ä»ç„¶å¯ä»¥è®°å½•å®Œæˆçš„å›åˆæ•°
            "train_solve_rate": train_solve_rate,
        })

        # è®­ç»ƒçŠ¶æ€æ‰“å°
        train_line = [
            f"[Train] loss {tr_stats_py['loss_total']:.3f}",
            f"V {tr_stats_py['loss_value']:.3f}", f"Ï€ {tr_stats_py['loss_policy']:.3f}",
            f"H {tr_stats_py['entropy']:.3f}",
            f"VÌ‚ {tr_stats_py['value_mean']:.3f}Â±{tr_stats_py['value_std']:.3f}",
            f"R {tr_stats_py['reward_mean']:.3f}Â±{tr_stats_py['reward_std']:.3f}",
            f"Ep {tr_stats_py['train_ep_num']}", f"Solve {tr_stats_py['train_solve_rate'] * 100:.1f}%",
        ]
        print("\n" + " | ".join(train_line))

        ev_stats_py = {"eval_solve_rate": 0.0, "eval_avg_len": float('inf'),
                       "eval_avg_return": 0.0, "eval_episodes": 0}

        # ===== 4. è¯„ä¼°ç­–ç•¥ (Evaluation) =====
        if tr_stats_py['train_solve_rate'] >= 0.70:  #
            key, sub_key = jax.random.split(key)
            problem_indices = jax.random.randint(sub_key, (EVAL_EPISODES_PER_CYCLE,), 0, len(problems_raw))
            key, ev_stats = evaluate_policy(
                key, env, network, train_state.params, problems,
                problem_indices=problem_indices,
                # num_episodes=EVAL_EPISODES_PER_CYCLE,
                max_steps=config.ENV_PARAMS.WRAPPER_PARAMS.max_steps
            )
            ev_stats_py = jax.device_get(ev_stats)

            eval_line = [f"[Eval ] Solve {ev_stats_py['eval_solve_rate'] * 100:.1f}%",
                         f"Len {ev_stats_py['eval_avg_len']:.2f}",
                         f"Return {ev_stats_py['eval_avg_return']:+.4f}",
                         f"Eps {ev_stats_py['eval_episodes']}"]
            print(" | ".join(eval_line))

            ev_solve, ev_len = ev_stats_py["eval_solve_rate"], ev_stats_py["eval_avg_len"]
            should_save = ev_solve > best_solve or (abs(ev_solve - best_solve) < 1e-9 and ev_len < best_len)

            if SAVE_BEST_EVERY_CYCLE and should_save:
                best_solve, best_len = ev_solve, ev_len
                checkpoints.save_checkpoint(
                    ckpt_dir=ckpt_dir, target=train_state, step=cycle_idx,
                    prefix="best_eval_", overwrite=True
                )
                print(f"âœ… New best (solve={best_solve * 100:.2f}%, len={best_len:.2f}). Saved.")
        else:
            print(f"| [Eval ] Skipped. Train solve rate {tr_stats_py['train_solve_rate'] * 100:.1f}% <= 80%")

        # ä¿å­˜å‘¨æœŸæ€§ checkpoint
        checkpoints.save_checkpoint(
            ckpt_dir=ckpt_dir, target=train_state, step=cycle_idx, prefix="cycle_", keep=3
        )

        # å†™å…¥æ—¥å¿—æ–‡ä»¶
        with open(log_path, "a", encoding="utf-8") as f:
            log_data = {**tr_stats_py, **ev_stats_py, "train_avg_len": 0.0, "train_avg_return": 0.0}  # è¡¥å…¨ç¼ºå¤±çš„è®­ç»ƒæŒ‡æ ‡
            f.write(
                f"{cycle_idx} | "
                f"{log_data['loss_total']:.6f} {log_data['loss_value']:.6f} {log_data['loss_policy']:.6f} {log_data['entropy']:.6f} | "
                f"{log_data['value_mean']:.6f} {log_data['value_std']:.6f} {log_data['reward_mean']:.6f} {log_data['reward_std']:.6f} | "
                f"{log_data['train_ep_num']} {log_data['train_solve_rate']:.6f} {log_data['train_avg_len']:.6f} {log_data['train_avg_return']:.6f} | "
                f"{log_data['eval_solve_rate']:.6f} {log_data['eval_avg_len']:.6f} {log_data['eval_avg_return']:.6f} {log_data['eval_episodes']}\n"
            )

        # æ›´æ–°è¿›åº¦æ¡
        pbar.set_postfix({
            "train_solve%": f"{tr_stats_py['train_solve_rate'] * 100:.1f}",
            "eval_solve%": f"{ev_stats_py['eval_solve_rate'] * 100:.1f}",
            "best_solve%": f"{best_solve * 100:.1f}"
        })

    # 5) ä¿å­˜æœ€ç»ˆæƒé‡
    checkpoints.save_checkpoint(
        ckpt_dir=ckpt_dir, target=carry.train_state, step=config.TRAIN_PARAMS.NUM_CYCLES,
        prefix="final_model_", overwrite=True
    )
    print(f"âœ… Final model saved to {ckpt_dir}")
    print(f"ğŸ“„ Full TXT log saved to {log_path}")


if __name__ == "__main__":
    train()