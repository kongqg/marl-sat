# æ–‡ä»¶: mappo_runner.py
import flax
import jax
import jax.numpy as jnp
import numpy as np
import os
import random
from tqdm import tqdm
from datetime import datetime
import hydra
from omegaconf import DictConfig, OmegaConf
from flax.training import checkpoints
from flax.training.train_state import TrainState
from functools import partial
import time
import optax

from src.envs.multi_agent_sat_env import SATEnv
from src.learners.mappo_gnn_sat_learner import SATDataWrapper, GNN_ActorCritic, make_train_cycle, RunnerState
from src.utils.data_parser import parse_cnf, load_cnf_problems
from src.utils.model_init import load_bc_and_inject


def set_global_seeds(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    return jax.random.PRNGKey(seed)


@partial(jax.jit, static_argnames=("env", "network", "max_steps"))
def evaluate_policy(key, env, network, params, problem_clauses, max_steps):
    key, reset_key = jax.random.split(key)
    obs, env_state = env.reset(problem_clauses, reset_key)

    def _one_step_eval(carry, _):
        env_state, obs, key = carry
        global_state = obs[1]  # obs is a tuple (local_obs, global_state)
        pi, _ = network.apply({'params': params}, global_state, env.agent_vars, env.action_mask)
        if env.action_mode == 0:  # single_flip
            # pi.logits çš„ shape æ˜¯ (num_agents, max_vars_per_agent + 1)
            # argmax ç›´æ¥å¾—åˆ°æ¯ä¸ª agent çš„æœ€ä½³åŠ¨ä½œç´¢å¼• (ä¸€ä¸ªæ•´æ•°)
            actions_array = jnp.argmax(pi.logits, axis=-1)  # shape: (num_agents,)
            actions_dict = {agent: actions_array[i] for i, agent in enumerate(env.agents)}
        else:  # multi_flip
            # pi.logits çš„ shape æ˜¯ (num_agents, max_vars_per_agent, 2)
            # å¯¹æ¯ä¸ªå˜é‡çš„ "ä¸ç¿»è½¬ vs ç¿»è½¬" logit å– argmax
            actions_array = jnp.argmax(pi.logits, axis=-1)  # shape: (num_agents, max_vars_per_agent)
            actions_dict = {agent: actions_array[i, :] for i, agent in enumerate(env.agents)}
        key, step_key = jax.random.split(key)
        next_obs, next_state, reward, done, info = env.step(key=step_key, state=env_state, actions=actions_dict)
        return (next_state, next_obs, key), (done, info, next_state.env_state.variable_assignments)

    _, (dones, infos, all_assignments) = jax.lax.scan(_one_step_eval, (env_state, obs, key), None, length=max_steps)

    # --- ä»¥ä¸‹æ˜¯ä¿®æ­£åçš„æ­£ç¡®é€»è¾‘ ---
    # 1. çœŸå®åœ°æ£€æŸ¥é—®é¢˜æ˜¯å¦åœ¨ä»»ä½•ä¸€æ­¥è¢«è§£å†³è¿‡
    was_ever_solved = jnp.any(infos['solved'])

    # 2. æ‰¾åˆ°ç¬¬ä¸€ä¸ªè§£å†³çš„æ­¥éª¤ç´¢å¼•ï¼Œè¿™ä¸ªç´¢å¼•åªæœ‰åœ¨ was_ever_solved ä¸º True æ—¶æ‰æœ‰æ„ä¹‰
    first_solve_step_index = jnp.argmax(infos['solved'])

    # 3. æ ¹æ®è¯¥ç´¢å¼•è·å–è§£
    solution = all_assignments[first_solve_step_index]

    # 4. åªæœ‰åœ¨çœŸæ­£è§£å†³æ—¶æ‰ä½¿ç”¨ solutionï¼Œå¦åˆ™ä½¿ç”¨å ä½ç¬¦
    no_solution_placeholder = jnp.zeros_like(solution)
    solution_assignments = jnp.where(was_ever_solved, solution, no_solution_placeholder)

    # 5. æ ¹æ® was_ever_solved æ¥è®¡ç®—çœŸå®çš„è§£å†³æ­¥æ•°
    steps_to_solve = jnp.where(was_ever_solved, first_solve_step_index + 1, max_steps)

    # 6. è¿”å›çœŸå®çš„è§£å†³çŠ¶æ€ã€æ­¥æ•°å’Œè§£
    return was_ever_solved, steps_to_solve, solution_assignments


@hydra.main(version_base=None, config_path="../../configs", config_name="MAPPO_CONFIG.yaml")
def main(hydra_config: DictConfig):
    config = OmegaConf.to_container(hydra_config, resolve=True)

    # --- 1. åˆå§‹åŒ–å’Œæ•¸æ“šåŠ è¼‰ ---
    seed = config.get('SEED', 42)
    key = set_global_seeds(seed)
    problems_raw = load_cnf_problems(config.get('CNF_DATA_DIR', 'data'))
    if not problems_raw:
        return

    # ã€æ–°ã€‘å°†é—®é¢˜åˆ—è¡¨è½¬æ¢ä¸º JAX PyTreeï¼Œæ–¹ä¾¿ä¼ å…¥ JIT å‡½æ•°
    # problemsç°åœ¨æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œæ¯ä¸ªé”®å¯¹åº”ä¸€ä¸ªåŒ…å«æ‰€æœ‰é—®é¢˜æ•°æ®çš„æ•°ç»„
    """
        # problems çš„æ ·å­:
        {
          'num_vars':    jnp.array([50, 50, 50, ...]),          # shape: (é—®é¢˜æ€»æ•°,)
          'num_clauses': jnp.array([212, 212, 212, ...]),       # shape: (é—®é¢˜æ€»æ•°,)
          'clauses':     jnp.array([<array_1>, <array_2>, ...]) # shape: (é—®é¢˜æ€»æ•°, æ¯ä¸ªé—®é¢˜çš„å­å¥æ•°, æ–‡å­—æ•°)
        }
    """

    print(f"[*] åŸå§‹å•é¡Œç¸½æ•¸: {len(problems_raw)}")
    # è¨­ç½®éš¨æ©Ÿæ•¸ç¨®å­ä»¥ä¿è­‰åŠƒåˆ†çš„å¯è¤‡ç¾æ€§
    np_rng = np.random.RandomState(seed)
    indices = np.arange(len(problems_raw))
    np_rng.shuffle(indices)

    split_idx = int(len(problems_raw) * 0.8)
    train_indices = indices[:split_idx]
    eval_indices = indices[split_idx:]

    train_problems_raw = [problems_raw[i] for i in train_indices]
    eval_problems_raw = [problems_raw[i] for i in eval_indices]

    print(f"[*] æ•¸æ“šé›†åŠƒåˆ†å®Œç•¢: {len(train_problems_raw)} å€‹ç”¨æ–¼è¨“ç·´ (80%), {len(eval_problems_raw)} å€‹ç”¨æ–¼è©•ä¼° (20%)")

    # å°‡è¨“ç·´é›†è½‰æ›ç‚º JAX PyTreeï¼Œæ–¹ä¾¿å‚³å…¥ JIT å‡½æ•¸
    numeric_train_problems_raw = [
        {"num_vars": p["num_vars"], "num_clauses": p["num_clauses"], "clauses": p["clauses"]}
        for p in train_problems_raw
    ]
    train_problems_tree = jax.tree_util.tree_map(lambda *x: jnp.stack(x), *numeric_train_problems_raw)
    # <--- æ ¸å¿ƒä¿®æ”¹ï¼šæ•¸æ“šé›†åŠƒåˆ† END --->
    # --- 2. å‡†å¤‡ç¯å¢ƒã€æ¨¡å‹å’Œè®­ç»ƒçŠ¶æ€ (åªæ‰§è¡Œä¸€æ¬¡) ---
    flat_config = {**config.get('environment', {}), **config.get('network', {}), **config.get('training', {})}

    # åˆ›å»ºåŸºç¡€ç¯å¢ƒå’ŒåŒ…è£…å™¨
    env = SATEnv(
        num_vars=flat_config["NUM_VARS"],
        num_clauses=flat_config["NUM_CLAUSES"],
        max_steps=flat_config["MAX_STEPS"],
        action_mode=flat_config["action_mode"],
        r_clause=flat_config["rewards"]["R_CLAUSE"],
        r_sat=flat_config["rewards"]["R_SAT"],
        gamma=flat_config["GAMMA"],  # è‹¥ SATEnv æ„é€ å‡½æ•°æ”¯æŒ
        vars_per_agent=flat_config["VARS_PER_AGENT"],
    )
    env = SATDataWrapper(env)

    # ã€æ–°ã€‘åˆ›å»ºå‘é‡åŒ–çš„ç¯å¢ƒå‡½æ•°
    vmapped_reset = jax.vmap(env.reset, in_axes=(0, 0))  # (problem_clauses, key)

    # åˆ›å»ºç½‘ç»œ
    network = GNN_ActorCritic(
        gnn_hidden_dim=flat_config['GNN_HIDDEN_DIM'],
        gnn_num_message_passing_steps=flat_config['GNN_NUM_MESSAGE_PASSING_STEPS'],
        num_agents=env.num_agents,
        max_vars_per_agent=env.max_vars_per_agent,
        action_mode=flat_config["action_mode"]
    )

    # åˆå§‹åŒ–ç½‘ç»œå‚æ•°å’Œ TrainState
    key, _rng_init, _rng_reset = jax.random.split(key, 3)
    # ä½¿ç”¨ç¬¬ä¸€ä¸ªé—®é¢˜ä½œä¸ºæ¨¡æ¿æ¥è·å– dummy_input çš„å½¢çŠ¶
    (dummy_local_obs, dummy_global_state), dummy_wrapper_state = env.reset(
        problems_raw[0]['clauses'], _rng_reset
    )
    dummy_clause_masks = dummy_wrapper_state.env_state.agent_clause_masks
    dummy_neighbor_masks = dummy_wrapper_state.env_state.agent_neighbor_masks
    network_params = network.init(
        _rng_init,
        dummy_global_state,  # <--- ä¹‹å‰æ˜¯ obs["gnn_input"]
        env.agent_vars,
        env.action_mask,
    )

    print("ğŸ” é¡å‹æª¢æŸ¥:")
    print(f"  - type(network_params): {type(network_params)}")
    if 'params' in network_params:
        print(f"  - type(network_params['params']): {type(network_params['params'])}")


    params_for_state = flax.core.freeze(network_params['params'])

    def create_lr_schedule(config):
        num_updates = config.get("NUM_UPDATES", 1)  # ç²å–ç¸½æ›´æ–°æ¬¡æ•¸

        # æª¢æŸ¥æ˜¯å¦å•Ÿç”¨é€€ç«
        if config.get("ANNEAL_LR", False):
            print("ğŸ“ˆ å•Ÿç”¨å­¸ç¿’ç‡ç·šæ€§é€€ç«...")
            lr_start = config.get("LEARNING_RATE", 3e-4) * config.get("LR_START_FACTOR", 1.0)
            lr_end = config.get("LR_END_FLOOR", 1e-5)

            # ä½¿ç”¨ optax çš„ linear_schedule å‰µå»ºé€€ç«å‡½æ•¸
            # å®ƒæœƒåœ¨ num_updates æ­¥å…§å¾ lr_start ç·šæ€§è¡°æ¸›åˆ° lr_end
            lr_schedule = optax.linear_schedule(
                init_value=lr_start,
                end_value=lr_end,
                transition_steps=num_updates
            )
            print(f"   - åˆå§‹å­¸ç¿’ç‡: {lr_start:.6f}")
            print(f"   - æœ€çµ‚å­¸ç¿’ç‡: {lr_end:.6f}")
            print(f"   - è¡°æ¸›æ­¥æ•¸: {num_updates}")
            return lr_schedule
        else:
            print("ğŸ“‰ æœªå•Ÿç”¨å­¸ç¿’ç‡é€€ç«ï¼Œä½¿ç”¨å›ºå®šå­¸ç¿’ç‡ã€‚")
            return config.get("LEARNING_RATE", 3e-4)

    # æ ¹æ“šé…ç½®å‰µå»ºå­¸ç¿’ç‡æˆ–å­¸ç¿’ç‡èª¿åº¦å™¨
    learning_rate_or_schedule = create_lr_schedule(flat_config)

    tx = optax.adam(learning_rate_or_schedule)
    # train_state = TrainState.create(apply_fn=network.apply, params=network_params, tx=tx)
    train_state = TrainState.create(apply_fn=network.apply, params=params_for_state, tx=tx)

    train_state_for_run = train_state  # é»˜è®¤ä½¿ç”¨æ–°åˆå§‹åŒ–çš„çŠ¶æ€

    continue_path = config.get('loading', {}).get('continue_rl_run_path')
    inject_path = config.get('loading', {}).get('inject_bc_model_path')

    if continue_path:
        # åœºæ™¯ä¸€ï¼šæ¢å¤ä¹‹å‰çš„ RL è®­ç»ƒ
        print(f"ğŸ”„ æ­£åœ¨å°è¯•ä» RL æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ: '{continue_path}'")
        ckpt_dir_to_load = os.path.join(continue_path, "checkpoints")
        try:
            loaded_state = checkpoints.restore_checkpoint(ckpt_dir=ckpt_dir_to_load, target=train_state,
                                                          prefix="latest_model_", step=0)
            if loaded_state:
                reset_optimizer = config.get('loading', {}).get('RESET_OPTIMIZER', False)
                if reset_optimizer:
                    train_state_for_run = train_state.replace(params=loaded_state.params)
                    print("âœ… RL æ£€æŸ¥ç‚¹åŠ è½½æˆåŠŸï¼æ¨¡å‹å‚æ•°å·²æ¢å¤ï¼Œä¼˜åŒ–å™¨å·²é‡ç½®ã€‚")
                else:
                    train_state_for_run = loaded_state
                    print("âœ… RL æ£€æŸ¥ç‚¹åŠ è½½æˆåŠŸï¼æ¨¡å‹å‚æ•°å’Œä¼˜åŒ–å™¨çŠ¶æ€å‡å·²æ¢å¤ã€‚")
            else:
                print(f"âš ï¸ è­¦å‘Š: ä» '{ckpt_dir_to_load}' åŠ è½½ RL æ£€æŸ¥ç‚¹å¤±è´¥ã€‚å°†ä»å¤´å¼€å§‹è®­ç»ƒã€‚")
        except Exception as e:
            print(f"âš ï¸ è­¦å‘Š: åŠ è½½ RL æ£€æŸ¥ç‚¹æ—¶å‘ç”Ÿé”™è¯¯: {e}ã€‚å°†ä»å¤´å¼€å§‹è®­ç»ƒã€‚")

    elif inject_path:
        # åœºæ™¯äºŒï¼šä» BC é¢„è®­ç»ƒæ¨¡å‹æ³¨å…¥æƒé‡
        print(f"ğŸ”„ æ­£åœ¨å°è¯•ä» BC é¢„è®­ç»ƒæ¨¡å‹æ³¨å…¥æƒé‡: '{inject_path}'")
        try:
            bc_checkpoint = checkpoints.restore_checkpoint(ckpt_dir=inject_path, target={'params': train_state.params},
                                                           prefix="bc_model_")
            if bc_checkpoint:
                bc_params = bc_checkpoint['params']
                new_params_mutable = train_state.params.unfreeze()

                # æ³¨å…¥ GNN Encoder å‚æ•°
                new_params_mutable['encoder'] = bc_params['encoder']
                print("  âœ… GNN Encoder å‚æ•°å·²æ³¨å…¥ã€‚")

                # æ³¨å…¥ Actor ç›¸å…³å‚æ•°
                actor_param_keys = [k for k in bc_params.keys() if 'actor' in k or 'agent_id_embedding' in k]
                for param_key in actor_param_keys:  # <--- å·²ä¿®æ­£
                    new_params_mutable[param_key] = bc_params[param_key]
                print(f"  âœ… Actor ç›¸å…³å‚æ•° ({len(actor_param_keys)} ä¸ª) å·²æ³¨å…¥ã€‚")

                print("  â„¹ï¸ Critic å‚æ•°å·²ä¿æŒéšæœºåˆå§‹åŒ–ã€‚")
                final_params = flax.core.freeze(new_params_mutable)

                # ä½¿ç”¨æ³¨å…¥çš„å‚æ•°å’Œå…¨æ–°çš„ä¼˜åŒ–å™¨çŠ¶æ€åˆ›å»º TrainState
                train_state_for_run = train_state.replace(params=final_params)
                print("ğŸ› ï¸  ä¼˜åŒ–å™¨çŠ¶æ€å·²è‡ªåŠ¨é‡ç½®ã€‚")
                print("ğŸ‰ BC é¢„è®­ç»ƒæ¨¡å‹è¿ç§»æˆåŠŸï¼")
            else:
                print(f"âš ï¸ è­¦å‘Š: ä» '{inject_path}' åŠ è½½ BC æ£€æŸ¥ç‚¹å¤±è´¥ã€‚å°†ä»å¤´å¼€å§‹è®­ç»ƒã€‚")
        except Exception as e:
            print(f"âš ï¸ è­¦å‘Š: åŠ è½½ BC æ£€æŸ¥ç‚¹æ—¶å‘ç”Ÿé”™è¯¯: {e}ã€‚å°†ä»å¤´å¼€å§‹è®­ç»ƒã€‚")
    else:
        # åœºæ™¯ä¸‰ï¼šä»å¤´å¼€å§‹è®­ç»ƒ
        print("â„¹ï¸ æœªé…ç½®ä»»ä½•åŠ è½½è·¯å¾„ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒã€‚")



    # --- 3. è·å– JIT ç¼–è¯‘çš„å•è½®è®­ç»ƒå‡½æ•° ---
    print("ğŸ› ï¸  æ­£åœ¨ JIT ç·¨è­¯å–®è¼ªè¨“ç·´å‡½æ•¸...")
    start_time = time.time()
    # è°ƒç”¨æ–°çš„ make_train_cycle å‡½æ•°
    train_cycle_fn = make_train_cycle(flat_config, env, network, vmapped_reset)
    print(f"âœ… JIT ç·¨è­¯å®Œæˆï¼è€—æ™‚: {time.time() - start_time:.2f} ç§’")

    # --- 4. æ—¥èªŒå’Œ Checkpoint è¨­ç½® ---
    time_str = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    run_dir = os.path.join(config.get('SAVE_DIR', 'experiments'), time_str)
    run_dir = os.path.abspath(run_dir)
    ckpt_dir = os.path.join(run_dir, "checkpoints")
    os.makedirs(ckpt_dir, exist_ok=True)
    print(f"ğŸ’¾ å®éªŒç»“æœå°‡ä¿å­˜åœ¨: {run_dir}")

    log_file_path = os.path.join(run_dir, "training_metrics.txt")

    # --- 5. æ‰§è¡Œè®­ç»ƒå¾ªç¯ ---
    print(f"\nğŸ‰ === å¼€å§‹è®­ç»ƒï¼Œæ€»å…± {flat_config['NUM_UPDATES']} è½®æ›´æ–° ===")
    start_time = time.time()




    # å‡†å¤‡åˆå§‹çš„ runner_state
    key, _rng = jax.random.split(key)
    # éšæœºé€‰æ‹©ä¸€æ‰¹åˆå§‹é—®é¢˜æ¥å¼€å§‹
    initial_indices = jax.random.randint(_rng, (flat_config["NUM_ENVS"],), 0, len(train_problems_raw))
    initial_problems = jax.tree_util.tree_map(lambda x: x[initial_indices], train_problems_tree)

    reset_keys = jax.random.split(_rng, flat_config["NUM_ENVS"])
    (initial_local_obs, initial_global_state), initial_env_state = vmapped_reset(initial_problems['clauses'], reset_keys)

    runner_state = RunnerState(
        train_state=train_state_for_run,
        env_state=initial_env_state,
        last_local_obs=initial_local_obs,
        last_global_state=initial_global_state,
        rng=key
    )
    with open(log_file_path, 'w', encoding='utf-8') as log_file:
        # å¯«å…¥æ—¥èªŒæ–‡ä»¶çš„æ¨™é ­ (ç¾åœ¨ä¸éœ€è¦ epoch é€™ä¸€åˆ—äº†)
        header = (
            "update,"
            "mean_return,solve_rate,avg_unsat_clauses,avg_solve_steps,explained_variance,"
            "value_loss,actor_loss,policy_entropy\n"
        )
        log_file.write(header)

        pbar = tqdm(range(flat_config['NUM_UPDATES']), desc="è®­ç»ƒè¿›åº¦")
        for update_idx in pbar:
            # åŸ·è¡Œä¸€è¼ª"æ•°æ®æ”¶é›† + æ¨¡å‹æ›´æ–°"
            runner_state, metrics = train_cycle_fn(runner_state, train_problems_tree, jnp.array(update_idx))
            metrics_cpu = jax.device_get(metrics)


            # 1. æº–å‚™æŒ‡æ¨™ï¼Œç›´æ¥å–æœ€å¾Œä¸€å€‹ epoch çš„æå¤±å€¼
            final_epoch_metrics = {
                "update": update_idx + 1,
                "return": metrics_cpu['mean_episodic_return'],
                "solve_rate%": metrics_cpu['solve_rate'] * 100,
                "unsat_C": metrics_cpu['avg_unsatisfied_clauses'],
                "solve_steps": metrics_cpu['avg_steps_to_solve'],
                "expl_var": metrics_cpu['explained_variance'],
                # ç›´æ¥ç´¢å¼• [-1] ä¾†ç²å–æ•¸çµ„çš„æœ€å¾Œä¸€å€‹å…ƒç´ ï¼Œä¸¦å–å¹³å‡å€¼
                "v_loss": metrics_cpu['epoch_value_losses'][-1].mean(),
                "a_loss": metrics_cpu['epoch_actor_losses'][-1].mean(),
                "entropy": metrics_cpu['epoch_entropies'][-1].mean(),
                "ent_coef": metrics_cpu['current_ent_coef'],
            }

            # 2. æ›´æ–°é€²åº¦æ¢é¡¯ç¤º
            pbar_display = {
                "update": final_epoch_metrics['update'],
                "return": f"{final_epoch_metrics['return']:.1f}",
                "solve%": f"{final_epoch_metrics['solve_rate%']:.1f}",
                "unsat_C": f"{final_epoch_metrics['unsat_C']:.1f}",
                "v_loss": f"{final_epoch_metrics['v_loss']:.3f}",
                "entropy": f"{final_epoch_metrics['entropy']:.2f}"
            }
            pbar.set_postfix(pbar_display)

            # 3. å¯«å…¥æ—¥èªŒæ–‡ä»¶
            log_line = (
                f"{final_epoch_metrics['update']},"
                f"{final_epoch_metrics['return']:.4f},{final_epoch_metrics['solve_rate%'] / 100:.4f},"
                f"{final_epoch_metrics['unsat_C']:.4f},{final_epoch_metrics['solve_steps']:.4f},"
                f"{final_epoch_metrics['expl_var']:.4f},"
                f"{final_epoch_metrics['v_loss']:.4f},{final_epoch_metrics['a_loss']:.4f},"
                f"{final_epoch_metrics['entropy']:.4f}\n"
            )
            log_file.write(log_line)

            if (update_idx + 1) % config.get('evaluation', {}).get('EVAL_INTERVAL', 10) == 0:
                pbar.write("-" * 50)
                pbar.write(f"ğŸ”¬ åœ¨ç·šè©•ä¼° (ä½¿ç”¨é©—è­‰é›†) [Update #{update_idx + 1}]")

                # å¾é©—è­‰é›†ä¸­éš¨æ©Ÿæ¡æ¨£ä¸€å°æ‰¹å•é¡Œ
                eval_batch_size = config.get('evaluation', {}).get('EVAL_BATCH_SIZE', 32)
                if len(eval_problems_raw) < eval_batch_size:
                    eval_batch = eval_problems_raw
                else:
                    eval_batch = random.sample(eval_problems_raw, k=eval_batch_size)

                eval_solved_count = 0
                temp_final_state = runner_state.train_state
                eval_keys_batch = jax.random.split(runner_state.rng, len(eval_batch))

                for j, problem in enumerate(eval_batch):
                    solved, _, _ = evaluate_policy(
                        eval_keys_batch[j], env, network, temp_final_state.params,
                        problem['clauses'], flat_config['MAX_STEPS']
                    )
                    if solved:
                        eval_solved_count += 1

                true_solve_rate = eval_solved_count / len(eval_batch)
                pbar.write(f"âœ… é©—è­‰é›†æ±‚è§£ç‡ ({len(eval_batch)}å€‹æ¨£æœ¬): {true_solve_rate:.2%}")
                pbar.write("-" * 50)

                pbar_display["eval_solve%"] = f"{true_solve_rate:.1%}"
                pbar.set_postfix(pbar_display)

            try:
                checkpoints.save_checkpoint(
                    ckpt_dir=ckpt_dir,
                    target=runner_state.train_state,
                    step=0,
                    prefix="latest_model_",
                    overwrite=True
                )
            except PermissionError as e:
                pbar.write(f"âš ï¸  è­¦å‘Š: ä¿å­˜æª¢æŸ¥é»å¤±æ•—ï¼Œæ¬Šé™è¢«æ‹’çµ•ã€‚å°‡ç¹¼çºŒè¨“ç·´...")
                pbar.write(f"   éŒ¯èª¤è©³æƒ…: {e}")


    end_time = time.time()
    print(f"ğŸ === è®­ç»ƒç»“æŸï¼æ€»è€—æ—¶: {end_time - start_time:.2f} ç§’ ===")

    # --- 6. ä¿å­˜æœ€ç»ˆæ¨¡å‹ ---
    print(f"ğŸ’¾ æœ€çµ‚æ¨¡å‹å·²ä¿å­˜è‡³: {ckpt_dir}")

    # --- 7. è®­ç»ƒåè¯„ä¼° ---
    eval_model_template = TrainState.create(apply_fn=network.apply, params=params_for_state, tx=tx)
    final_train_state = checkpoints.restore_checkpoint(
        ckpt_dir=ckpt_dir,
        target=eval_model_template,
        step=0,
        prefix="latest_model_"
    )

    print(f"âœ… å·²åŠ è½½æœ€æ–°çš„æ¨¡å‹")
    print("\nğŸ”¬ === å¼€å§‹æµ‹è¯•æ•´ä¸ªæ•°æ®é›† ===")

    # --- 7a. è®¾ç½®è¯„ä¼°å’Œè§£å†³æ–¹æ¡ˆæ—¥å¿—è®°å½• ---
    solutions_file_path = os.path.join(run_dir, "test_solutions.txt")
    print(f"ğŸ“„ å°†è§£å†³æ–¹æ¡ˆè®°å½•åˆ°: {solutions_file_path}")

    solved_count = 0
    total_steps_to_solve = 0
    # ä½¿ç”¨åŒ…å«åç§°çš„åŸå§‹é—®é¢˜åˆ—è¡¨
    eval_problems = eval_problems_raw
    key, eval_key_base = jax.random.split(runner_state.rng)
    eval_keys = jax.random.split(eval_key_base, len(eval_problems))

    # --- 7b. è¿è¡Œè¯„ä¼°å¹¶å°†æ—¥å¿—è®°å½•åˆ°æ–‡ä»¶ ---
    with open(solutions_file_path, 'w', encoding='utf-8') as solutions_file:
        pbar_eval = tqdm(enumerate(eval_problems), total=len(eval_problems), desc="è©•ä¼°é€²åº¦")

        for i, problem in pbar_eval:
            # è·å–å¯¹åº”çš„ JAX éšæœºå¯†é’¥
            eval_key = eval_keys[i]

            solved, steps_to_solve, solution = evaluate_policy(
                eval_key, env, network, final_train_state.params,
                problem['clauses'], flat_config['MAX_STEPS']
            )

            if solved:
                solved_count += 1
                total_steps_to_solve += steps_to_solve

                # å°†å¸ƒå°”è§£è½¬æ¢ä¸º 0/1 å­—ç¬¦ä¸²
                solution_np = np.array(solution)
                solution_str = "".join(solution_np.astype(np.int32).astype(str))

                # ä½¿ç”¨ pbar_eval.write æ‰“å°åˆ°æ§åˆ¶å°ï¼Œä»¥å…æ‰°ä¹±è¿›åº¦æ¡
                pbar_eval.write(f"âœ… å•é¡Œ {problem['name']} å·²è§£æ±ºï¼æ­¥æ•¸: {steps_to_solve}")

                # å°†ç»“æœå†™å…¥è§£å†³æ–¹æ¡ˆæ–‡ä»¶
                solution_line = f"Problem: {problem['name']}, Solved: True, Steps: {steps_to_solve}, Solution: {solution_str}\n"
                solutions_file.write(solution_line)
            else:
                # å°†å¤±è´¥ç»“æœå†™å…¥è§£å†³æ–¹æ¡ˆæ–‡ä»¶
                solution_line = f"Problem: {problem['name']}, Solved: False\n"
                solutions_file.write(solution_line)

            # æ›´æ–°è¿›åº¦æ¡çš„åç¼€æ˜¾ç¤º
            pbar_eval.set_postfix(
                {"solve_rate": f"{solved_count / (i + 1):.2%}"}
            )

    # --- 7c. æ‰“å°æœ€ç»ˆæ‘˜è¦ ---
    avg_steps = total_steps_to_solve / solved_count if solved_count > 0 else 0
    solve_rate = solved_count / len(eval_problems)
    print(f"\nâœ… Final Solve Rate on {len(problems_raw)} problems: {solve_rate:.2%}")
    print(f"ğŸ“Š Average steps to solve (for solved problems): {avg_steps:.2f}")


if __name__ == "__main__":
    main()