FUNCTION train_cycle(θ, env_states, problems_dataset):
    // θ: 当前模型参数
    // env_states: B 个并行环境的当前状态

    // ==========================================================
    //  1. 并行数据收集 (Rollout)
    // ==========================================================
    
    // B个环境并行执行 T 步 (NUM_STEPS)
    FOR t = 0 TO T-1:
        // 根据当前观测 s_t，采样动作 a_t
        a_t ~ π_θ(·|s_t)

        // 环境执行动作，获得下一状态、奖励和结束信号
        s_{t+1}, r_t, done_t ← env.step(a_t)

        // 若有环境结束，则用一个新的随机问题立刻重置它
        IF done_t THEN s_{t+1} ← env.reset_with_new_problem()
    
    // 收集 T 步的轨迹数据 D = {s_t, a_t, r_t, V_θ(s_t), log π_θ(a_t|s_t)}

    // ==========================================================
    //  2. 计算优势和目标 (GAE)
    // ==========================================================

    // 从后往前 (t = T-1 ... 0) 遍历轨迹数据 D
    FOR t = T-1 DOWNTO 0:
        // a) 计算 TD-Error
        δ_t ← r_t + γ * V(s_{t+1}) * (1 - done_t) - V(s_t)

        // b) 递归计算 GAE
        Â_t ← δ_t + γ * λ * Â_{t+1} * (1 - done_t)

        // c) 计算价值目标
        V_t^{target} ← Â_t + V(s_t)

    // ==========================================================
    //  3. PPO 模型更新
    // ==========================================================
    
    // 对收集到的数据进行 E 轮 (UPDATE_EPOCHS) 更新
    FOR epoch = 1 TO E:
        // 将数据 D 切分成多个微批次 (minibatches)
        FOR each minibatch in D:
            // a) 计算策略比例
            ratio_t ← π_θ(a_t|s_t) / π_{θ_old}(a_t|s_t)

            // b) 计算总损失 L_total
            L_CLIP ← -min(ratio_t * Â_t,  clip(ratio_t, 1-ε, 1+ε) * Â_t)
            L_VF   ← (V_θ(s_t) - V_t^{target})^2
            S      ← Entropy(π_θ(·|s_t))

            L_total ← mean(L_CLIP + c₁ * L_VF - c₂ * S)
            
            // c) 使用 Adam 根据梯度更新模型参数
            θ ← θ - η * ∇_θ L_total

    // ==========================================================
    //  4. 返回更新后的状态
    // ==========================================================
    RETURN updated_θ, final_env_states
